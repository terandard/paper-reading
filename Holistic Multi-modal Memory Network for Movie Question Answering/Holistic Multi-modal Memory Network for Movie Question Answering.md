Anran Wang, Anh Tuan Luu, Chuan-Sheng Foo, Hongyuan Zhu, Yi Tay, Vijay Chandrasekhar  
[arXiv](https://arxiv.org/abs/1811.04595) , [pdf](https://arxiv.org/pdf/1811.04595.pdf)  

# どんなもの？
MovieQAデータセットの最先端の精度を実現

# 既存研究との違い
既存のアプローチでは、1つのアテンションホップでデータソース間の部分的なやり取りしか使用しません。  
本稿では、各ホップにおける異なる入力ソース（マルチモーダルコンテキスト、質問）間の相互作用を完全に考慮する Holistic Multi-modal Memory Network（HMMN）フレームワークを提示する。
さらに、文脈検索段階では、回答選択を考慮に入れる。

## abstract
したがって、提案されたフレームワークは、マルチモーダルコンテキスト、質問、および回答情報を効果的に統合し、質問応答のためにより情報的なコンテキストを取得する。大規模なアブレーション研究は、ホリスティックな推論と異なるアテンション戦略の貢献の重要性を示しています。

## introduction
本研究では、ムービーの質問応答（MovieQA）[26]に焦点を当てる。このシステムでは、映画から取ったビデオや字幕に関連した複数の選択肢の質問にうまく答えて、ストーリーの理解を示すシステムが必要となる。  
マルチモーダルQAの重要課題は、さまざまなデータソースの情報を統合することです。 MovieQAのコンテキストでは、クエリとコンテキストのアテンション（query-to-context attention）と、ビデオと字幕の間のモーダル間のアテンション（inter-modal attention between videos and subtitles）が考慮されるべきです。 最近開発された方法は、両者に限界がある初期灌流（earlyfusion [19]）および後期融合（late-fusion [26]）の古典的戦略を採用している。 後者の融合は、モダリティ間の相互参照が高レベルの意味的特徴を定義することを可能にしないが、異なるモダリティの早期融合は、特徴レベルにおけるノイズの増加により有意義な意味相関を拾う能力を制限し得る。 Wangら[28]は、MovieQAのためにモーダル間の注意を利用することを提案した。 しかしながら、彼らの方法は、異なる注意ステージが、質問、ビデオ、コンテキスト検索のための字幕の間の相互作用の異なるサブセットを考慮する入力データを完全には統合しない。  

# 提案モデル

## notation
詳細は[28]に記述．  
$S\in \mathbb{R}^{d\times m}$ を字幕モダリティとする．$d$ は特徴ベクトルの次元，$m$ は字幕文の数．字幕モダリティについては、関連するビデオクリップ内の字幕文を収集するだけでなく、文脈情報を利用するために時間的に近い字幕文を組み込む。各単語のword2vec表現（$d_w$ 次元）を射影行列 $W_1\in\mathbb{R}^{d_w\times d}$ で $d$ 次元に射影する．次に、各文のすべての単語の間に平均プールが実行され、文の表現が得られます。  
$V\in\mathbb{R}^{d\times n}$ をビデオモダリティとする．$d$ は特徴ベクトルの次元，$n$ はフレーム数．各質問に対して関連するビデオクリップから固定数のフレームを選択する．フレームレベル表現は、語彙における領域特徴と単語表現との間のアテンションを調査することによって生成される．ここで $W_2\in\mathbb{R}^{d_r\times d_w}$ は、単語表現の次元に合致するようにVGGの領域特徴を $d_w$ 次元に投影するために利用される。$d_r$ は領域特徴の次元．平均プールが行われ、続いてW1が投影される。  
質問と回答の選択肢は、字幕文と同じ方法で表されます。 質問はベクトル $q\in\mathbb{R}^{d}$ として表される。各質問に対する解答選択肢は、$A = [a_0,a_1,a_2,a_3,a_4]\in\mathbb{R}^{d\times5}$ として表され、各解答選択は $a_k\in\mathbb{R}^d$ として符号化される。 $W_1$ と $W_2$ のみが学習可能である。 図2（a）に、字幕と映像のモダリティの表現を生成する構造を示す。

## end-to-end memory network
End-to-end Memory Network（E2EMN）[25]は、本来、文脈の文脈に従って答えとして語彙から最も可能性の高い単語を選ぶことである質問応答タスクのために提案されている。 [26]において、E2EMNは、複数選択回答を用いた多モード質問応答に適合している。 特に、2つのモダリティからのスコアは、最終的な予測を行うために後で融合されます。 E2EMNはテキストの質問応答のために設計されているため、この方法は単一のモダリティからのコンテキストのみを扱います。 ここでは、説明のために字幕モダリティSを使用します。  
E2EMNでは、コンテキストSはメモリスロットとして扱われます。 メモリスロットとクエリを入力として、クエリとメモリスロットの間の関連性に従ってコンテキストの要約が導出される。 特に、クエリqと各メモリスロットとの間の一致度は、内積とそれに続くソフトマックスで計算される。

$\alpha_i = softmax(q^TS_{:i})$  

ここで、$\alpha_i$ は $i$ 番目の字幕文の重要度を示す。要約された文脈 $u$ は、$\alpha_i$ に基づく字幕文特徴の加重和として計算される。

$u=\sum_{i=1}^m \alpha_i S_{:i}$

次に、回答候補 $a_i$ を $q$ と $u$ の和と比較することによって予測する．

$p=softmax((q+u)^TA)$

ここで $p\in\mathbb{R}^5$ は信頼ベクトル．

## HMMN
HMMNフレームワークは、E2EMNと異なり、マルチモーダルコンテキストを入力として扱います。 HMMNフレームワークは、マルチモーダルコンテキストと質問との相互作用を共同で調査します。 これにより、コンテキスト間のマルチモーダル関係をモデリングしながら、クエリとコンテキストの関係を共同で検討します。 さらに、回答予測のための回答選択だけでなく、複数のモダリティからの文脈を要約するプロセスにおいても活用されます。  
推論プロセスは、HMMNセルと呼ばれる小さなビルディングブロックを積み重ねることによって実行されます。 HMMNセルの構造を図2（a）に示す。 主な貢献はHMMN構造にあります。 各HMMNセルは、質問、1つの回答選択肢、ビデオおよび字幕からのコンテキストを入力として受け取り、応答認識の要約コンテキストを導出する。 私たちは、このプロセスを推論の1つのホップと呼んでいます。 $u_t^k$ を、回答選択肢 $k$ に関する $t$ 番目の推論ホップの出力とする。 $t$ 番目のホップの出力は、$（t +1）$ 番目のホップの入力として利用される。

1）コンテキスト検索における回答の関与：HMMNセルは、コンテクスト検索段階でクエリの一部として回答選択を組み込む。 前ホップ $u$ の出力、質問 $q$、解答選択 $a_k$ を組み合わせて、$t$ 番目のホップに対する $k$ 番目の解答選択を含むクエリが計算される：

$q^* = u_{t-1}^k + a_k +\lambda q$

$\lambda$ は質問と残りのクエリとの間のトレードオフパラメータである。  
文脈が長くて複雑な場合、質問に答えるためのすばやく効果的な方法は、各回答の選択肢に関する関連情報を見つけることです。 1つの回答選択に対して、検索された回答対応コンテキストが回答選択肢と同様の考えを伝える場合、それは正解である傾向があります。 あるいは、検索されたコンテキストが異なる意味的意味を有する場合、その答えは間違っている可能性が高い。

2）各ホップにおける異なるアテンションメカニズムをホリスティックに検討：我々のフレームワークは、クエリとマルチモーダルコンテキストの間の相互作用のサブセットのみを取るのではなく、各ホップにおけるモーダル間およびクエリ間のアテンション戦略を共同で検討する。
HMMNセルは、質問、回答選択、ビデオ、字幕の間の相互作用が全体的に利用されるマルチモーダルコンテキストから記述的情報を収集するために、クエリ $q^*$ をとる。 具体的には、更新されたクエリを使用して、$S$ の関連するサブタイトルセンテンスを照会からコンテキストへのアテンション($q^*\rightarrow S_{:i}$)として実行する。 得られた再加重サブタイトルモダリティは、$S^*$ として表される:

$\delta_i = softmax(q^{*T}S_{:i})$

$S_{:i}^*=\delta_iS_{:i}$

より関連性の高い字幕文が大きな重みと関連している。

モダリティ間の注意推論は、ビデオモダリティVを使用して、字幕モダリティ $S$ ($(V\rightarrow S^*)$ として示される）に関与することによって適用され、フレームに対する字幕認識表現を $V^*$ として生成することを目的とする。 各フレームは、関連性に従ってすべての字幕文の特徴の加重和で表される。

$\varepsilon_{ij} = V_{:i}^T S_{;j}^*$

$V_{:i}^* = \sum_{j=1}^m \varepsilon_{ij}S_{:j}^*$

得られた $V^*$ は、クエリ $q^*$ をホップ出力として要約することができる。 特に、$k$ 番目の解答選択に関する $t$ ホップ目の要約文脈は、$u_t^k$ として示される。

$\zeta_i=softmax(q^{*T}V_{;i}^*)$

$u_t^k=\sum_{i=1}^n \zeta_i V_{:i}^*$

各推論ホップでは、前のホップの出力、回答選択、質問、マルチモーダルコンテキストが全体的に統合されています。 Vを使用してSにアテンションする理由は、字幕モダリティがMovieQAタスクのビデオモダリティよりも有益であるためです。 典型的には、字幕モダリティは、キャラクタリレーションシップ、ストーリー展開などのストーリーの記述を含む。 Sにアテンションすることによって、S内の特徴表現を用いて要約文脈を形成する。 再加重されたSは、字幕モダリティのより有益な表現を導出するための情報スクリーニングステップとして機能する。

3）親和性スコアによる回答の予測：元のE2EMNには、複数のホップの設定が改善された結果が示されています。 我々は推論のTホップを行うためにHMMNセルを積み重ねる。 解答選択 $a_k$ に関する最終的な要約文脈 $u_T^k$ が与えられると、$a_k$ に対する親和性スコア $f_k$ が生成される。 このスコアは、質問とanswer-awareの要約されたコンテキストと回答の選択肢の合計を以下のように比較することによって得られます:

$f_k=(q+u_T^k)^Ta_k$

このスコアは、検索された文脈が回答選択と一貫したセマンティック意味を有するかどうかを示す。 親和性スコアを生成する構造を図2（b）に示す。 次に、すべての解答選択肢の親和性スコアをsoftmax関数に渡して、図2（c）に示すような最終的な解答予測を得て、クロスエントロピー損失を標準確率勾配降下で最小化する。 これは、1つの回答選択肢が回答認識要約文脈と一致する場合、正解である可能性が高いことを示します。

# 評価実験
Movie QAデータセットを使用．

字幕モダリティについては、ビデオクリップの開始時点と終了時点を300秒延長することによって得られる時間間隔に入る字幕文を考慮する。 ビデオモダリティについては、[28]に続く関連ビデオクリップから32フレームが選択される。 我々は、[26]によって提供されるword2vec表現を使用する。 word2vec表現dwの次元は300です。VGG-16 drの 'pool5'からの地域特徴の次元は512です。バッチサイズは8，学習率は0.005，トレードオフパラメータλは0.45，特徴dの次元は300に設定される。我々のモデルは50エポックまで訓練され、早期停止が実行される。


## アテンションと層の数を変更
![table1]  
コンテキスト検索の段階で回答の選択肢を組み込むことで、パフォーマンスが大幅に向上する．  
2層構造が最良の性能を達成するので、ホップ数Tである層の数も2に設定される。

## 質問別の性能比較
![fig3]  

## 他の手法との比較
![table2]  
特に[26]は後期融合を行う。 [19]は早期融合を行う。 後期融合も早期融合も、モダリティ間の関係をうまく利用することができず、結果は最適ではない。 [13]はエンドツーエンドの訓練可能ではない。 [14]は、視覚的表現とテキスト表現のシーケンスが同じ長さであると仮定しますが、これはMovieQAでは当てはまりません。 [19]と同様に、[14]は字幕を伴わずにそれらのフレームの視覚的情報を遮断する。 [28]は、マルチモーダル関係を考慮しているが、各アテンションステージでは、質問、ビデオ、字幕の間の相互作用の異なるサブセットが考慮される。 これと比較して、HMMNフレームワークの注意は、以前のホップの出力、各ホップの質問、ビデオ、字幕を全体的に組み込み、優れたパフォーマンスにつながります。


## 別のアテンションメカニズムの検討
(i)Query-to-context Attention ($q\rightarrow S$) , $S'$  
どのメモリ・スロットがクエリにより関連しているかを示す．字幕モダリティ $S$ をイラストレーションのコンテキストとして使用．式1のクエリと各メモリスロットとの間の計算された類似性を用いる．  

$S'_{:i} = \alpha_iS_{:i}$

(ii)inter-modal Attention ($S\rightarrow V$) , $\bar{S}$  
各字幕文について、最も関連性の高いフレームを見つける  

$\beta_{ij}=S_{:i}^T V_{:j}$

$\bar{S}_{:i} = \sum_{j=1}^n \beta_{ij} V_{ij}$

(iii) Intra-modal Self Attention ($S\rightarrow S$) , $\hat{S}$
質問応答や機械翻訳などのタスクで効果的なアテンション（[8],[27]）．直観は、他のメモリスロット間のコンテキスト情報が悪用される可能性があることです。モード間の注意と同様に、同じモダリティ内の異なるメモリスロット間の干渉は、次のように計算される。

$\gamma_{ij} = I(i\ne j)S_{:i}^T S_{:j}$

$\hat{S}_{:i} = \sum_{j=1}^m \gamma_{ij}S_{:j}$

![table3]  
E2EMN内のメモリスロットとしてアテンションを扱い，その答えを予測．  
$S$ はキャラクタの関係とストーリー展開に関するより有益な記述を含んでいるため，$V$ より優れている．  

![table4]  
$V\rightarrow S$ アテンションを使用すると性能が上がる．

## 定性的評価
![fig4]  
上の例では，回答のアテンションを使用しなかった場合，「Apple」という言葉が字幕に記載されていないため２つ目の選択肢を選択してしまう．  

![fig5]  

# 議論はあるか
失敗例から常識的な知識やオブジェクトの検出結果を活用する必要がある．  
ConceptNet のような知識グラフを活用し，ネットワークに高レベルの意味情報を加える．

# conclusion
マルチモーダルデータからの文脈で質問に答えることを学ぶ HMMN フレームワークを発表．提案手法では，マルチモーダルコンテキストと質問との間の相互作用を共同でモデル化するために、モーダル間(inter-modal)およびクエリ間(query-to-context)の両方のアテンションメカニズムを調べる。 さらに、コンテキスト検索の段階で回答の選択肢を組み込むことで、回答のアテンションを探ります。
将来のモデル設計のためのガイダンスを提供する可能性のある様々なアテンションメカニズムの詳細なアブレーション試験を提示した。