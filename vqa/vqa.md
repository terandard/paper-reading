Aishwarya Agrawal, Jiasen Lu, Stanislaw Antol,
Margaret Mitchell, C. Lawrence Zitnick, Dhruv Batra, Devi Parikh
ICCV 2015
[URL](http://www.visualqa.org/) , [pdf](https://arxiv.org/pdf/1505.00468v6.pdf)

## VQAとは？
画像と画像に関する質問文を受け取り，正確な自然言語の回答を導出するタスク．  
VQAで成功するシステムとは，一般的なイメージキャプションを生成するシステムよりも，画像と複雑な推論の理解が必要となる．  
VQAは視覚障害のあるユーザや情報アナリストが視覚情報を引き出す手助けをすることを目的とする．  
VQAは自動定量評価にも適しており,この作業の進捗状況を効果的に追跡することができる．  
我々の提案されたタスクは，人間によって提供された自由形式の質疑応答を含む．私たちの目標は，正しい答えを提供するために必要な知識の多様性と推論の種類を増やすこと．


## 背景
コンピュータビジョン(CV)，自然言語処理(NLP)，知識表現と推論(KR)を組み合わせた画像とビデオのキャプションの研究が増加している．

画像キャプションが所望のように「AI完全」でない可能性があることを示唆している．
筆者らは次世代のAIアルゴリズムを生み出すための理想的なタスクとは以下の要素が必要と考えている．
（i）単一のサブドメイン（CVなど）を超えたマルチモーダルな知識が必要
（ii）進捗状況を追跡するための明確な定量的評価指標がある

画像キャプションなどのタスクでは，自動評価は依然として困難でオープンな研究課題となっている．


## データセットについて
画像はリアル画像とアブストラクト画像がある．

・リアル画像 : [MS COCOデータセット](http://cocodataset.org/#home)からの204,721個の画像を使用

・アブストラクト画像 : 
5万シーンを含む新しく作成された抽象シーンデータセットを作成．
このデータセットには、20種類の「ペーパードール」人間モデル[2]、性別、人種、年齢の異なる8種類の表現が含まれています。クリップアートは、屋内と屋外の両方のシーンを描写するために使用することができる。このセットには、100以上のオブジェクトと31種類の動物がさまざまなポーズで含まれています。 このクリップアートを使用することで、以前の論文よりも実際のイメージをより密接に反映するより現実的なシーンを作成することができます（図2の下段を参照）
すべての抽象的なシーンに対して5つのシングルキャプションを収集

## question
データセットには760,000件以上の質問が含まれ，約10M件の回答があります。
質問に正しく答えるには画像が必要であり，常識的な情報だけでは答えることができないような質問を用意する必要がある．(例：図1の質問「口髭は何で作られている？」)
多種多様な質問タイプと難易度を持つことで、視覚的理解と常識的推論の両方の継続的な進歩を測定することができます。

オープンエンドの質問と選択式の質問がある．
自由形式の応答を必要とするオープンエンドのタスクとは異なり、多肢選択タスクでは、事前定義された可能な回答リストからアルゴリズムを選択するだけで済みます。

オープンエンドの質問には以下のものが含まれている
・細かい認識が必要なもの（例：ピザには何のチーズがあるか？）
・物体検出が必要なもの（例：何台のバイクがあるか？）
・アクティビティの認識が必要なもの（例：この男性は泣いているか？）
・知識ベースの推論が必要なもの（例：これは野菜のピザですか？）
・常識的な推論が必要なもの（例：この人は20/20のビジョンを持っていますか？）

## answer
多くの質問には単純な　"Yes" , "No" で十分．  
一部の質問には短いフレーズが必要な場合があり，複数の異なる回答が正しいこともある．例：“white”,“tan”,“off-white” は、すべて同じ質問に対する正解である可能性がある．  
これに対応するために，各質問ごとに 10 件の人間による回答を収集した．

テストのために、私たちは質問に答えるための2つのモダリティを提供しています：（i）オープンエンドと（ii）選択式
オープンエンドタスクの場合、生成された回答は、次の精度メトリックを使用して評価されます。

```math
accuracy = min(\frac{\# humans\ that\ provided\ that\ answer}{3},1)  
```
つまり少なくとも3人の労働者が正確な答えを出していれば、100％の正確さが得られます。  
VQAではほとんどの回答（89.32％）は1語なので，回答した文字列が真値に一致しているかで判断する．   


多肢選択のタスクでは、各質問ごとに18の候補回答が作成される．無制限のタスクと同様に、選択されたオプションの精度は、その回答を提供した人間の被験者の数に基づいて計算されます（3で割って1でクリップします）。 
4組の回答から正解と誤答の候補セットを生成します。

各画像には最低3つ質問があり、各質問には10の回答がある．
回答の種類にはバリエーションがあり，アノテーターが最も多く回答したもの(Correct)，画像を見ないで回答したもの(Plausible)，よくある回答(Popular: 「2つ」「yes」など)，回答からランダムに選択したものなどがある．

# data analysis
質問の種類と提供される回答の種類を理解するために，質問の種類と回答の分布を視覚化．  
常識的な情報だけを使用して、画像なしで質問をどのくらいの頻度で答えられるかを調べます。  
画像キャプションに含まれる情報が質問に答えるのに十分であるかどうかを分析します。

## question
質問を開始する単語に基づいて質問のタイプを分類．最初の4単語に基づく分布を示す．  
![fig3]  

定量的な質問の割合  
![table3]  
多様な質問タイプが存在

質問文の長さ  
![fig4]  
ほとんどの質問は4語から10語  

## answer
質問のタイプに対する回答の分布  
![fig5]  
“Is the. . . ”, “Are. . . ”, “Does. . . " の質問には "yes" , "no" が多い．  
他の質問の回答にはバリエーションがある．  
ほとんどの回答は1単語であり，単語数1,2,3 それぞれの割合は リアル画像で 89.32％ , 6.91％ , 2.74％，抽象的なシーンで 90.51％ , 5.89％ , 2.49％である．  
現実の画像のデータセットには23,234個のユニークな1単語の回答があり，抽象的なシーンには3,770個のユニークな回答がある．  

実際の画像と抽象的なシーンのそれぞれの質問の38.37％と40.66％は "yes" , "no" で答えられる．‘yes/no’ の質問では 'yes' に偏っている（実際の画像の 58.83％，抽象的なシーンの 55.86％が 'yes'）．
実際の画像と抽象的なシーンの質問の12.31％と14.48％が数字で回答する質問．数字の回答において実際の画像の26.04％，抽象的なシーンの39.85％が "2"．

## 人間の回答結果
![table1]  
画像なしでは正答率が低くなるので，正しく回答するためには画像の情報が必要である．


## ベースライン
いくつかのベースラインと斬新な方法を使用して、MS COCO画像のVQAデータセットの難しさを調べます。

### ベースライン
１．random : VQA train/valデータセットの上位1Kの回答から無作為に回答を選択
２．prior ("yes") : 常に「yes」と回答
３．per Q-type prior : オープンエンドのタスクでは，最も一般的な質問タイプごとの回答を選択．複数選択タスクでは，Word2Vecの特徴空間でコサイン類似度を使用して，オープンエンドタスクの選択された回答に最も類似する回答を選択．
４．nearest neighbor : テスト画像と質問が与えられると、まず、K個の最も近い質問および関連する画像をトレーニングセットから見つける。 次に、オープンエンドのタスクでは、この最も近い質問と画像セットから最も頻繁に出現する真値の回答を選択する。複数選択タスクの場合は「per Q-type prior」と同様




オープンエンド型質問の使用には多くの利点がありますが、質問するタイプのタイプや、さまざまなアルゴリズムが回答するのに適しているタイプを理解することは、依然として有効です。 この目的のために、尋ねられる質問のタイプと提供される回答のタイプを分析します。 いくつかの視覚化を通じて、私たちは尋ねられる質問の驚異的な多様性を実証します。 また、質問の情報内容と回答が画像キャプションとどのように異なるかを調べます。 ベースラインについては、テキストと最先端のビジュアルフィーチャの両方を組み合わせた手法をいくつか提案しています[29]。 VQAイニシアチブの一環として、我々は最先端の方法とベストプラクティスについて議論するための年1回の挑戦と関連ワークショップを開催する。


