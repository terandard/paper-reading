Yash Goyal , Tejas Khot , Douglas Summers-Stay , Dhruv Batra , Devi Parikh  
Virginia Tech , Army Research Laboratory , Georgia Institute of Technology , CVPR2017  
[arXiv](https://arxiv.org/abs/1612.00837) , [pdf](https://arxiv.org/pdf/1612.00837.pdf)


# どんなもの？
VQAデータセット v1.0 を拡張した v2.0 を作成．  
v1.0 では言語バイアスが強かった．  
v2.0 では，同じ質問に対する異なる画像と回答のペアを作り，データセットを拡張する．  
また，モデルが質問に答える際に概念を理解していることをユーザに信頼してもらうために，反例画像を出力するモデルを提案している．

![fig1]


# dataset
VQAデータセットを拡張する．
VQAの実画像データセットは、[MS COCOデータセット](http://cocodataset.org/#home)から204,721枚の画像，61万件の質問（画像につき3つの質問），および600万以上の自由形式の回答（質問ごとに10回答）を含む．このデータセットは言語バイアスが強い．

VQAデータセットのすべての（画像$I$，質問$Q$，回答$A$）について，質問$Q$の答えが $A'$（$A$とは異なる）になる $I$ と似た画像 $I'$ を識別できるようにしたい．  

画像の特徴ベクトルを用いて距離を計算し，24個の類似した画像を集める．  
Amazon Mechanical Turk（AMT）を用いて，$I$ に似ている 24 の画像，質問$Q$ ，回答$A$に対し，回答が$A$と異なる画像$I'$を集めた．  
10人の新しいAMT労働者に質問$Q$をして画像$I'$を表示し，回答10件を収集する．10件の中で最も一般的な答えが$A'$．  

完全なバランスの取れたデータセットには，train : 443K，val : 214K，test : 453Kの（質問，画像）ペアが含まれている．

![fig2]  

回答の分布の比較．  
![fig4]  


# 評価実験
既存の最先端のVQAモデルを使用．
データセットのバランスを修正しているので，質問に正しく答える学習をするだけで，画像情報に焦点を当てるはず．

## 使用したモデル
- Deeper LSTM Question + norm Image (d-LSTM+nI)[24]:  
画像のCNN埋込み，質問のLSTM埋め込みを使用し，ポイントワイズ乗算で組み合わせ，多層パーセプトロン分類器を組み合わせて回答の確率分布を予測する．

- Hierarchical Co-attention（HieCoAtt）[25]：  
画像と質問の両方に 'Co-attention' して回答を予測．具体的には単語レベル，フレーズレベル，全体の質問レベルで階層的に質問をモデル化する．これらのレベルは再帰的に組み合わされ，回答の確率分布を生成．

- Multimodal Compact Bilinear Pooling (MCB) [9]:  
VQAチャレンジ2016で１位を獲得したモデル．このモデルでは MCB メカニズムを使用して画像特徴にアテンションし，アテンションした画像特徴と言語特徴を結合する．

- Prior:  
トレーニングセットで最もよく出てきた回答をする．

- Language-only:  
d-LSTM+nI と似たアーキテクチャで言語情報のみ使用．

## 実験結果
![table1]  
UU と UB を比較すると，UB の方が性能が悪い．このことから言語バイアスを学習しており，同じ質問が異なる画像上で異なる回答を有する場合に質問に正しく答えることができていない．  
同じ質問に対する異なる画像と回答のペアを正しく予測できているか確認する．
HieCoAtt モデルでは U で学習した際に，ペアの13.5％を正しく予測でき，59.9％が同じ予測を持ち，40.1％が異なる予測を持っていた．  
B で学習すると，ペアの17.7％を正しく予測し，パフォーマンスが4.2％向上した．49.4％が同じ予測をしたが，10%低くなっている．これは B の学習によって似ている2つの画像の違いを学習していることを示している． 

![table2]  

![table3]  
Yes/No では言語バイアスが強かったので，UB では精度が悪い．
不均衡なVQAデータセットでは最先端のVQAモデルに同様の精度をもたらしているので，画像情報も考慮している良いモデルなのかが区別できない．

# counter-example
質問に答えるだけでなく，反例画像も出力するモデル．  
質問Q、解答A、およびモデルが反例を識別しなければならない集合INNが入力として与えられる。

反例画像を介して回答Aを説明することを学習する．
QI と A を線形変換する2チャネルネットワーク．これらの2つの内積を全結合層に通してK個のスコアS（Ii）を生成する．

$L = -\log P(A|I,Q) + \lambda \sum_i max(0,M - (S(I') - S(I_i)))$

前半がクロスエントロピー，後半がペアワイズヒンジ損失関数

## result
![table4]  
Random:ランダムに選択  
Distance:元画像からの距離でソートし，類似しているものを選択  
VQA: P(A|Q, Ii)を用いて，INNからQへの答えとしてAを持つ可能性が最も低い画像を選択  
画像から意味のある詳細を抽出し，理解できるビジュアルモデルはまだ少ない．

