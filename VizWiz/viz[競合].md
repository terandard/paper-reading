VizWiz Grand Challenge: Answering Visual Questions from Blind People
Danna Gurari, Qing Li, Abigale J. Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, Jeffrey P. Bigham  
CVPR2018  
[arXiv](https://arxiv.org/abs/1802.08218) , [pdf](https://arxiv.org/pdf/1802.08218.pdf)  


# どんなもの？
視覚障害のある人が撮影した写真とそれに関する会話から作成した新しいVQAデータセット VizWiz を作成．  
![fig1]

# 先行研究との差分
既存の VQA データセットは人工的につくられたものであるので，本来の目的に即していない．  


# abstract
視覚的質問に自動的に答えるためのアルゴリズムの研究は現在、人工的なVQA設定で構築された視覚的質問応答（VQA）データセットによって動機付けられています。我々は、自然なVQA設定から生じる最初の目標指向VQAデータセットであるVizWizを提案します。 VizWizは視覚障害者から発せられた10のクラウドソーシングされた答えと共に、それぞれが携帯電話を使って写真を撮ってそれについて話された質問を記録した盲目の人々から発する31,000以上の視覚的質問から成ります。 VizWizは、多くの既存のVQAデータセットとは異なります。
（1）画像は盲目の写真家によって撮影されているため、品質が低いことがよくあります。
（2）質問が話され、会話が活発になる。
（3）視覚的な質問には答えられないことが多い。視覚的な質問に答え、視覚的な質問に答えることができるかどうかを判断するための最新のアルゴリズムを評価すると、VizWizはやりがいのあるデータセットであることがわかります。このデータセットを導入して、視覚障害者を支援することができる、より一般化されたアルゴリズムの開発をより大きなコミュニティに奨励します。

# introduction
視覚応用のための望ましい次のステップは、視覚障害者が周囲の物理的世界について知りたいことを自然な方法で直接要求することを可能にすることです。このアイデアは、一般的なビジュアル質問応答（VQA）問題に対する最近の関心の高まりに関連しています。これは、あらゆる画像に関するあらゆる質問に正確に答えることを目的としています。

過去3年間で、VQA問題に関する研究を促進するために、多くのVQAデータセットがビジョンコミュニティに登場しました。 歴史的に見れば、与えられたコンピュータビジョン問題に関する研究コミュニティの進歩は典型的に大規模な、公に共有されたデータセットによって先行されています。 ただし、利用可能なVQAデータセットの制限は、すべて人工的に作成されたVQA設定から来ることです。 そのうえ、盲人から来るイメージや質問に対して「目標指向」なものはありません。 それでも、盲目の人々はおそらくアルゴリズムを訓練するのに望ましいビッグデータを作り出してきた。 10年近くの間、盲目の人々は写真を撮る[4、9]と彼らが撮る写真について質問する[9、12、27]。 さらに、盲目の人々はしばしば彼らの本当の日々のニーズをサポートするためのコンピュータビジョンツールの初期の採用者です。

視覚障害者の利益にも取り組む、より一般化されたアルゴリズムの開発を促進するために、視覚障害者から発せられた最初の公的に利用可能なビジョンデータセットを紹介します。これを「VizWiz」と呼びます。
私たちの仕事は、写真を撮ってそれについて質問することによって盲人が7万人を超える視覚的な質問をするのをサポートする携帯電話アプリケーションを確立した以前の仕事[9]を基にしています。私たちは、関連する個人の安全性やプライバシーを危険にさらす可能性のある視覚的な質問をすべて削除するための厳格なフィルタリングプロセスを実装することから始めました。
盲目の人々はしばしば個人的な障害を克服するために他人と積極的に個人情報を共有します[5]。その後、クラウドソースが回答してアルゴリズムのトレーニングと評価をサポートします。次に、画像、質問、回答を特徴付け、VizWizを既存の多くのVQAデータセットと区別する独自の側面を明らかにするための実験を行います。最後に、答えを予測するための多数のアルゴリズム[18、24]と、視覚的な質問に答えられるかどうかを予測するためのアルゴリズム[30]を評価します。私たちの調査結果は、VizWizが最新のビジョンアルゴリズムにとって難しいデータセットであることを強調し、VQA問題に関する新しい展望を提供します。


なぜVizWizが現代のアルゴリズムに対して挑戦的であるのかを理解することも有用です。 我々の調査結果は、VizWizが視覚障害者からの画像や質問、そして最初に話された質問を紹介する最初のビジョンデータセットであるという事実に由来する理由を示唆しています。 既存のビジョンデータセットとは異なり、画像は、明るさ、焦点のずれ、関心のあるコンテンツのフレーミングが原因で、画質がよくありません。 既存のVQAデータセットとは異なり、質問は会話的であるか、どちらかの端で質問をクリッピングしたり、バックグラウンドのオーディオコンテンツを聞き取ったりするなどのオーディオ録音の不完全さの影響を受ける可能性があります。 最後に、盲目の人々は自分のイメージが彼らが求めているビジュアルコンテンツを捉えていることを確認できないので、質問に答えられるという保証はありません。 例えば、ぼけ、不適切な照明、レンズを指で覆うことなど。前述の問題のいくつかは、図１に例示されている。

もっと広く言えば、VizWizはVQAシステムの実際のユーザーの現実世界の利益を捉える最初の目標主導型VQAデータセットです。 さらに、人が自分の周りの物理的な世界について質問するユースケースを反映した最初のVQAデータセットです。 このアプローチは、視覚障害者が日々の視覚に基づく課題を克服できるようにするために重要です。 自動化された方法を開発することに成功すれば、視覚障害に答えるために人間に頼るという盲目の人々にとっての今日の現状からの多くの望ましくない結果についての懸念を軽減するだろう[9、12、27]。 例えば、人間はしばしば支払われなければならず（すなわち、潜在的に高価）、答えを提供するのに数分かかる（すなわち遅く）、常に利用可能ではない（すなわち潜在的にスケーラブルではない）、そしてプライバシー問題を提起する（例えばクレジットカード情報） 共有されています）。



# VizWiz: Dataset Creation
「VizWiz」と呼ばれるVQAデータセットを紹介します。これは、現実の世界で盲人が尋ねる視覚的な質問で、日常的な視覚的な質問に対する回答を求めています。このデータセットは、iPhoneおよびAndroid携帯電話プラットフォームで利用可能なアプリケーションであるVizWizモバイルアプリケーション1を使用して、4年間で72,205の視覚的な質問が発生した以前の研究[9]に基づいています。視覚障害者はこのアプリケーションを使って、毎日の視覚的なアクセシビリティの課題について尋ねました[9、11]。ある人が写真を撮ってから話し言葉を録音することによって視覚的な質問をしました。 VizWizアプリケーションは2011年5月にリリースされ、11,045人のユーザーによって使用されています。収集された視覚的質問のうち48,169件は、視覚的質問を匿名で共有することに同意したユーザーから寄せられました。これらの視覚的な質問は、私たちのデータセットを開発するための出発点として役立ちます。このセクションでは、VizWizで視覚的な質問をするためのアプローチと、既存の多くのVQAデータセットを構築するために使用されているアプローチとを比較します。その後、データセットの作成方法について説明します。

## Visual Question Collection Analysis
![table1]  
表1に、VizWizの視覚的な質問を収集するプロセスが、既存の14のVQAデータセットに採用されているプロセスとどのように異なるのかをまとめます。明確な違いは、VizWizには盲目の写真家からの画像が含まれていることです。そのような画像の品質は、大量の画像のぼやけ、不十分な照明、および不十分な画像コンテンツのフレーミングなど、既存のデータセットでは通常見られない課題を提供します。
もう1つの違いは、質問が会話(口語)であることです。技術と話すことはますます彼らの技術（例えば、AppleのSiri、Google Now、AmazonのAlexa）を持つ人々のための標準的な相互作用アプローチとなりつつあり、VizWizはより多くの会話言語と音声録音エラーのようなこの質問をする様相から生じる新しい挑戦を生み出します。
もう1つの違いは、VizWizは、ある人が写真を撮ってからそれについて質問する最初のデータセットです。これは、視覚的な質問が、モバイルデバイスで探求されている周囲の環境についての人々の日々の興味を反映する、新しいユースケースシナリオを反映しています。他のすべてのVQAデータセットとは対照的に、質問をする人々は画像を「見る」ことができなかったので、VizWizもユニークです。その結果、図1に例示されているさまざまな理由から、質問は画像とは無関係になることがよくあります。

## Collecting Answers
次に、最後の一連の33,373の視覚的な質問に対する回答を収集しました。 オリジナルのVizWizアプリケーションは、回答へのほぼリアルタイムのアクセスを人に提供することを優先し、その人がクラウドワーカー、IQエンジン、Facebook、Twitter、または電子メールから回答を受け取ることを許可しました。 私たちの目的はアルゴリズムの訓練と評価を可能にすることであるので、私たちはこの目的のためにすべての視覚的な質問に対する新しい答えを集めました。

回答を集めるために、VQA 1.0の作成に使用された優れたプロトコルを修正しました[8]。 以前と同様に、米国にいるAMTのクラウドワーカーから，質問とそれに関連したイメージを見せて、彼らに「完全な文章ではなく、短いフレーズ」を返すように指示することによって 視覚的な質問1件につき10の回答を集めました。

また、「画像の品質が低すぎて質問に答えることができない（つまり、すべて白、すべて黒、またはぼやけすぎている）」場合に “Unsuitable Image” と回答する手順を追加しました
または「画像から質問に答えることができない」場合は “Unanswerable” と回答する
その結果、答えは英語を見て流暢な素人から基本的な常識をとらえます。 さらに、答えは視覚的な質問が答えられるかどうかを明らかにします。

# VizWiz: Dataset Analysis
このセクションの目的は、VizWizの視覚的な質問と回答を評価することです。 分析する
（1）自然言語の質問の多様性は何ですか？、
（2）画像の多様性は？
（3）答えの多様性は？
（4）視覚的な質問はどれほどの頻度で解決できないのでしょうか。 この分析の有益な結果は、実際のVQA設定における盲目のユーザーの利益についての私たちの理解を豊かにすることです。

## Analysis of Questions
![fig2]  
VizWizはまれな最初の単語で始まることが多いことがわかります。 実際、全質問の5％未満で発生する最初の単語から始まる質問の割合は、Viz Wizでは24.2％、VQA 2.0では13.4％です[8]（40,000のVQのランダムなサブセットに基づく）。この発見は、質問をするときにより会話的な言葉を使ったことに一部起因しています。e.g., “Hi”, “Okay”, and “Please”
図2では、ほとんどの質問は「What」で始まっています。 これは、VizWizの最初の言葉は、もっともらしい答えの範囲を狭める（そして答えに意味のある方法を使用する）という点で貧弱な仕事をすることが多いことを示唆しています。 対照的に、「How many…」や「Is…」などの最初の文言は、都合よく数字に対する妥当な答えを狭め、「はい/いいえ」と答えます。 それでも、「はい/いいえ」と「数」の回答は、VizWizの視覚的な質問の2.02％と1.65％にすぎません。

また、各質問の単語数を要約した統計を計算することによって、質問の多様性を分析します。質問の長さの中央値と平均長はそれぞれ6語と8語で、25番目と75番目の百分位数の長さはそれぞれ4語と8語です。我々の調査結果は、[14]と[22]でうまくまとめられた既存の人工的に構築されたVQAデータセットで見つけられた統計が実際に観察された統計と一致することを示します。
また、“What is this?”という3つの単語でも質問に十分であることがよくあります。 図2からわかるように、この短い物体認識の質問は最も一般的な質問です。 
長い質問や多文の質問も時折発生します。これは、通常、人々が望ましい回答を明確にするために補助的な情報を提供するためです。
e.g., “Which one of these two bags would be appropriate for a gift? The small one or the tall one? Thank you.” 
音声記録装置があまりにも多くのコンテンツをキャプチャしたり、バックグラウンドの音声コンテンツをキャプチャしたりすると、より長い質問も発生する可能性があります。
“I want to know what this is. I’m have trouble stopping the recordings.”

## Analysis of Answers
![fig4]  
図4では、答え「Unwewerable」,「Unsuitable Image」を除いた多数の(∼58,789)固有回答があることを示している．
絶対数で言うと既存の大規模データセットよりも一桁小さいが，既存のデータセットとの重複は少ない．VizWizの上位3,000回答のうち824のみがVQA2.0の上位3,000回答に含まれている．
また質問の27.9％が答えられないこと(「Unwewerable」,「Unsuitable Image」)がわかった．

また、各回答の単語数を要約した統計を計算して、回答の多様性を分析します。 中央値と平均回答長はそれぞれ1.0語と1.8語です。
これは人工的に構築された多数のVQAデータセットについて観察された回答統計量が実際に観察されるものと似ていることを示しています。 また、52.1％が1つの単語、32.9％が2つの単語、8％が3つの単語、3.4％が4つの単語、残りの3.6％が4つ以上の単語を持つという回答の割合も計算します。
この矛盾は、VizWizの視覚的な質問で、複数の単語のテキストを読むように要求されることが多いという観察結果に一部起因しています。

# VizWiz Benchmarking
既存のアルゴリズムに対するVizWizデータセットの難しさを調査します。データセットをトレーニング、検証、テストセットそれぞれ20,000、3,173、および8,000の質問に分割します（つまり、およそ65/10/25に分割）。

## Visual Question Answering
![table3]  
![table4]  
