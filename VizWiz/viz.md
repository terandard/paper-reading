VizWiz Grand Challenge: Answering Visual Questions from Blind People
Danna Gurari, Qing Li, Abigale J. Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, Jeffrey P. Bigham  
CVPR2018  
[arXiv](https://arxiv.org/abs/1802.08218) , [pdf](https://arxiv.org/pdf/1802.08218.pdf) , [HP](http://vizwiz.org/data/) 


# どんなもの？
視覚障害のある人が撮影した写真とそれに関する質問から作成した新しいVQAデータセット VizWiz を作成．  
![fig1]

# 先行研究との差分
VQAチャレンジの目的は視覚障害のあるユーザに対して視覚情報を提供するのを手助けすること．  
既存の VQA データセットは人工的につくられたものであるので，本来の目的に即していない．  
VizWiz は視覚障害のある人が撮影した写真とそれに関する質問から作成している．  
- 画像はピントが合っていなかったり，暗いなど品質が悪いものも含まれている．  
- 質問は口語的なものもある．
- 質問に答えることが出来ないもの(“Unsuitable Image”, “Unanswerable”)もある．


# abstract
視覚的質問に自動的に答えるためのアルゴリズムの研究は現在、人工的なVQA設定で構築された視覚的質問応答（VQA）データセットによって動機付けられています。我々は、自然なVQA設定から生じる最初の目標指向VQAデータセットであるVizWizを提案します。 VizWizは視覚障害者から発せられた10のクラウドソーシングされた答えと共に、それぞれが携帯電話を使って写真を撮ってそれについて話された質問を記録した盲目の人々から発する31,000以上の視覚的質問から成ります。 VizWizは、多くの既存のVQAデータセットとは異なります。
（1）画像は盲目の写真家によって撮影されているため、品質が低いことがよくあります。
（2）質問が話され、会話が活発になる。
（3）視覚的な質問には答えられないことが多い。視覚的な質問に答え、視覚的な質問に答えることができるかどうかを判断するための最新のアルゴリズムを評価すると、VizWizはやりがいのあるデータセットであることがわかります。このデータセットを導入して、視覚障害者を支援することができる、より一般化されたアルゴリズムの開発をより大きなコミュニティに奨励します。

# introduction
視覚応用のための望ましい次のステップは、視覚障害者が周囲の物理的世界について知りたいことを自然な方法で直接要求することを可能にすることです。このアイデアは、一般的なビジュアル質問応答（VQA）問題に対する最近の関心の高まりに関連しています。これは、あらゆる画像に関するあらゆる質問に正確に答えることを目的としています。

過去3年間で、VQA問題に関する研究を促進するために、多くのVQAデータセットがビジョンコミュニティに登場しました。 歴史的に見れば、与えられたコンピュータビジョン問題に関する研究コミュニティの進歩は典型的に大規模な、公に共有されたデータセットによって先行されています。 ただし、利用可能なVQAデータセットの制限は、すべて人工的に作成されたVQA設定から来ることです。 そのうえ、盲人から来るイメージや質問に対して「目標指向」なものはありません。 それでも、盲目の人々はおそらくアルゴリズムを訓練するのに望ましいビッグデータを作り出してきた。 10年近くの間、盲目の人々は写真を撮る[4、9]と彼らが撮る写真について質問する[9、12、27]。 さらに、盲目の人々はしばしば彼らの本当の日々のニーズをサポートするためのコンピュータビジョンツールの初期の採用者です。

視覚障害者の利益にも取り組む、より一般化されたアルゴリズムの開発を促進するために、視覚障害者から発せられた最初の公的に利用可能なビジョンデータセットを紹介します。これを「VizWiz」と呼びます。
私たちの仕事は、写真を撮ってそれについて質問することによって盲人が7万人を超える視覚的な質問をするのをサポートする携帯電話アプリケーションを確立した以前の仕事[9]を基にしています。私たちは、関連する個人の安全性やプライバシーを危険にさらす可能性のある視覚的な質問をすべて削除するための厳格なフィルタリングプロセスを実装することから始めました。
盲目の人々はしばしば個人的な障害を克服するために他人と積極的に個人情報を共有します[5]。その後、クラウドソースが回答してアルゴリズムのトレーニングと評価をサポートします。次に、画像、質問、回答を特徴付け、VizWizを既存の多くのVQAデータセットと区別する独自の側面を明らかにするための実験を行います。最後に、答えを予測するための多数のアルゴリズム[18、24]と、視覚的な質問に答えられるかどうかを予測するためのアルゴリズム[30]を評価します。私たちの調査結果は、VizWizが最新のビジョンアルゴリズムにとって難しいデータセットであることを強調し、VQA問題に関する新しい展望を提供します。


なぜVizWizが現代のアルゴリズムに対して挑戦的であるのかを理解することも有用です。 我々の調査結果は、VizWizが視覚障害者からの画像や質問、そして最初に話された質問を紹介する最初のビジョンデータセットであるという事実に由来する理由を示唆しています。 既存のビジョンデータセットとは異なり、画像は、明るさ、焦点のずれ、関心のあるコンテンツのフレーミングが原因で、画質がよくありません。 既存のVQAデータセットとは異なり、質問は会話的であるか、どちらかの端で質問をクリッピングしたり、バックグラウンドのオーディオコンテンツを聞き取ったりするなどのオーディオ録音の不完全さの影響を受ける可能性があります。 最後に、盲目の人々は自分のイメージが彼らが求めているビジュアルコンテンツを捉えていることを確認できないので、質問に答えられるという保証はありません。 例えば、ぼけ、不適切な照明、レンズを指で覆うことなど。前述の問題のいくつかは、図１に例示されている。

もっと広く言えば、VizWizはVQAシステムの実際のユーザーの現実世界の利益を捉える最初の目標主導型VQAデータセットです。 さらに、人が自分の周りの物理的な世界について質問するユースケースを反映した最初のVQAデータセットです。 このアプローチは、視覚障害者が日々の視覚に基づく課題を克服できるようにするために重要です。 自動化された方法を開発することに成功すれば、視覚障害に答えるために人間に頼るという盲目の人々にとっての今日の現状からの多くの望ましくない結果についての懸念を軽減するだろう[9、12、27]。 例えば、人間はしばしば支払われなければならず（すなわち、潜在的に高価）、答えを提供するのに数分かかる（すなわち遅く）、常に利用可能ではない（すなわち潜在的にスケーラブルではない）、そしてプライバシー問題を提起する（例えばクレジットカード情報） 共有されています）。



# VizWiz: Dataset Creation
携帯電話プラットフォームで利用可能なアプリケーションであるVizWizモバイルアプリケーションを使用して，4年間で72,205の視覚的な質問を収集した以前の研究[9](http://up.csail.mit.edu/other-pubs/vizwiz.pdf)に基づいている．ある視覚障害者が写真を撮ってから話し言葉を録音することによって視覚的な質問を収集した． VizWizアプリケーションは2011年5月にリリースされ，11,045人のユーザーによって使用されている．収集された視覚的質問のうち48,169件は、，覚的質問を匿名で共有することに同意したユーザーから寄せられた．

## Visual Question Collection Analysis
![table1]  
既存のデータセットとの違いは，VizWizには視覚障害者からの画像が含まれていること．そのような画像の品質は，大量の画像のぼやけ，不十分な照明など既存のデータセットでは通常見られない課題を提供する．そして質問は画像とは無関係になることがよくある．  
また，質問が会話(口語)である．AppleのSiri、Google Now、AmazonのAlexa のような日常的な口語の質問を必要とする技術の開発を手助けする．

## Collecting Answers
33,373の視覚的な質問に対する回答を収集した． 
回答を集めるために，VQA 1.0の作成に使用されたプロトコルを修正して使用した．米国にいるAMTのクラウドワーカーから，質問とそれに関連したイメージを見せて，彼らに「完全な文章ではなく，短いフレーズ」を返すように指示することによって，視覚的な質問1件につき10の回答を集めた．

また、「画像の品質が低すぎて質問に答えることができない」場合に “Unsuitable Image” と回答し，「画像から質問に答えることができない」場合は “Unanswerable” と回答する手順を追加した．

# VizWiz: Dataset Analysis

## Analysis of Questions
![fig2]  
VizWizはまれな最初の単語で始まることが多い．実際、全質問の5％未満で発生する最初の単語から始まる質問の割合は、Viz Wizでは24.2％、VQA 2.0では13.4％です[8]（40,000のVQのランダムなサブセットに基づく）。この発見は、質問をするときにより会話的な言葉を使ったことに一部起因しています。e.g., “Hi”, “Okay”, and “Please”  

ほとんどの質問は「What」で始まっています。 これは、VizWizの最初の言葉は、もっともらしい答えの範囲を狭める（そして答えに意味のある方法を使用する）という点で貧弱な仕事をすることが多いことを示唆しています。 対照的に、「How many…」や「Is…」などの最初の文言は、都合よく数字に対する妥当な答えを狭め、「はい/いいえ」と答えます。 それでも、「はい/いいえ」と「数」の回答は、VizWizの視覚的な質問の2.02％と1.65％にすぎません。

各質問の単語数を要約した統計を計算することによって、質問の多様性を分析します。質問の長さの中央値と平均長はそれぞれ6語と8語で、25番目と75番目の百分位数の長さはそれぞれ4語と8語です。我々の調査結果は、[14]と[22]でうまくまとめられた既存の人工的に構築されたVQAデータセットで見つけられた統計が実際に観察された統計と一致することを示します。
また、“What is this?”という3つの単語でも質問に十分であることがよくあります。 図2からわかるように、この短い物体認識の質問は最も一般的な質問です。 
長い質問や多文の質問も時折発生します。これは、通常、人々が望ましい回答を明確にするために補助的な情報を提供するためです。
e.g., “Which one of these two bags would be appropriate for a gift? The small one or the tall one? Thank you.” 
音声記録装置があまりにも多くのコンテンツをキャプチャしたり、バックグラウンドの音声コンテンツをキャプチャしたりすると、より長い質問も発生する可能性があります。
“I want to know what this is. I’m have trouble stopping the recordings.”

## Analysis of Answers
![fig4]  
54,253のユニークな回答がある．絶対数で見ると既存のデータセットより少ないが，重複は少ない．VizWizの上位3,000回答のうち824のみがVQA 2.0の上位3,000回答に含まれている．

視覚的な質問の27.9％が答えられないことがわかった
![fig10]

回答単語の分布は，67.32が1つの単語，20.74%が2つの単語，8.24%が3つの単語，3.52%が4つの単語，残りが4つ以上の単語

# VizWiz Benchmarking
## Visual Question Answering
既存手法
- Making the V in VQA Matter[18](https://arxiv.org/abs/1612.00837)
- A Strong Baseline For Visual Question Answering[24](https://arxiv.org/abs/1704.03162)
- Bottom-Up and Top-Down Attention for VQA[6](https://arxiv.org/abs/1707.07998)

![table3]  
![table4]  


## Visual Question Answerability
アルゴリズムが視覚的な質問をどれだけ正確に回答可能であるか、または画像と質問だけを使用しないで分類できるかという問題に取り掛かります。

- Q+C : 質問が画像に関連しない際に使われる手法[30](https://arxiv.org/abs/1705.00601)
- FT
- VizWiz
- VQA
- Q : question alone
- C : caption alone
- I : image alone using the ResNet-152 CNN features
- Q+I : the question with the image

![fig5]
AP score of 30.6 for [30] VS 71.7 for Q+I  
この調査結果は低品質の画像に起因するものであり、多くの場合 unanswerable となっている　　
